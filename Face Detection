B1: Get Image From Video 
import cv2
import time

def main():
    cap = cv2.VideoCapture('../video/Quang_01.mp4')
    # Resolution 640*480
    time.sleep(1)
    if cap is None or not cap.isOpened():
        print('Khong the mo file video')
        return
    cv2.namedWindow('Image', cv2.WINDOW_AUTOSIZE);
    n = 1
    dem = 200
    while True:
        [success, img] = cap.read()
        ch = cv2.waitKey(30)
        if success:
            #img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)
            imgROI = img[0:(0+480),:] # Tao ra anh 480x480

            imgROI = cv2.resize(imgROI,(250,250))
            cv2.imshow('Image', imgROI)
        else:
            break
        if n%4 == 0:
            filename = '../image/BanQuang/BanQuang_%04d.bmp'%(dem)
            cv2.imwrite(filename,imgROI)
            dem = dem + 1
        n = n + 1
    return

if __name__ == "__main__":
    main()
    
    
B2: Training
from model import create_model
import numpy as np
import os.path
import cv2
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import joblib

from align import AlignDlib
from sklearn.metrics import f1_score, accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import LinearSVC
from sklearn.manifold import TSNE


class IdentityMetadata():
    def __init__(self, base, name, file):
        # dataset base directory
        self.base = base
        # identity name
        self.name = name
        # image file name
        self.file = file

    def __repr__(self):
        return self.image_path()

    def image_path(self):
        return os.path.join(self.base, self.name, self.file) 
    
def load_metadata(path):
    metadata = []
    for i in sorted(os.listdir(path)):
        for f in sorted(os.listdir(os.path.join(path, i))):
            # Check file extension. Allow only jpg/jpeg' files.
            ext = os.path.splitext(f)[1]
            if ext == '.jpg' or ext == '.jpeg' or ext == '.bmp':
                metadata.append(IdentityMetadata(path, i, f))
    return np.array(metadata)

def load_image(path):
    img = cv2.imread(path, 1)
    # OpenCV loads images with color channels
    # in BGR order. So we need to reverse them
    return img[...,::-1]

def align_image(img):
    return alignment.align(96, img, alignment.getLargestFaceBoundingBox(img), 
                           landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)


def distance(emb1, emb2):
    return np.sum(np.square(emb1 - emb2))

def show_pair(idx1, idx2):
    plt.figure(figsize=(8,3))
    plt.suptitle(f'Distance = {distance(embedded[idx1], embedded[idx2]):.2f}')
    plt.subplot(121)
    plt.imshow(load_image(metadata[idx1].image_path()))
    plt.subplot(122)
    plt.imshow(load_image(metadata[idx2].image_path()))


nn4_small2_pretrained = create_model()
nn4_small2_pretrained.load_weights('weights/nn4.small2.v1.h5')


metadata = load_metadata('../image')

# Initialize the OpenFace face alignment utility
alignment = AlignDlib('models/shape_predictor_68_face_landmarks.dat')

# Load an image of a person
# xem ngươi thu 140
jc_orig = load_image(metadata[10].image_path())

# Detect face and return bounding box
bb = alignment.getLargestFaceBoundingBox(jc_orig)

# Transform image using specified face landmark indices and crop image to 96x96
jc_aligned = alignment.align(96, jc_orig, bb, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)

# Show original image
plt.subplot(131)
plt.imshow(jc_orig)

# Show original image with bounding box
plt.subplot(132)
plt.imshow(jc_orig)
plt.gca().add_patch(patches.Rectangle((bb.left(), bb.top()), bb.width(), bb.height(), fill=False, color='red'))


# Show aligned image
plt.subplot(133)
plt.imshow(jc_aligned)

plt.show()

embedded = np.zeros((metadata.shape[0], 128))
TTD_size = metadata.shape[0]

# Sai ở đây

dem = 0

for i, m in enumerate(metadata):
    print(m.image_path())
    img = load_image(m.image_path())
    img = align_image(img)
    if img is not None:
        # scale RGB values to interval [0,1]
        img = (img / 255.).astype(np.float32)
        # obtain embedding vector for image
        embedded[i] = nn4_small2_pretrained.predict(np.expand_dims(img, axis=0))[0]
        dem = dem + 1

if dem < TTD_size:
    while True:
        mm, nn = embedded.shape
        flagThoat = True
        for i in range(0, mm):
            if np.sum(embedded[i]) == 0:
                embedded = np.delete(embedded, i, 0)
                metadata = np.delete(metadata, i, 0)
                flagThoat = False
                break
        if flagThoat == True:
            break
print(embedded.shape)
print(metadata.shape)
# xem cung 1 nguoi
# 20 và 40 la 1 nguoi
show_pair(20, 40)
# xem 2 nguoi khac nhau
# 20 và 400 la 2 nguoi khac nhau
show_pair(20, 400)
plt.show()

distances = [] # squared L2 distance between pairs
identical = [] # 1 if same identity, 0 otherwise

num = len(metadata)
for i in range(num - 1):
    for j in range(i + 1, num):
        distances.append(distance(embedded[i], embedded[j]))
        identical.append(1 if metadata[i].name == metadata[j].name else 0)
        
distances = np.array(distances)
identical = np.array(identical)

thresholds = np.arange(0.02, 0.4, 0.005)

f1_scores = [f1_score(identical, distances < t) for t in thresholds]
acc_scores = [accuracy_score(identical, distances < t) for t in thresholds]

opt_idx = np.argmax(f1_scores)
# Threshold at maximal F1 score
opt_tau = thresholds[opt_idx]
# Accuracy at maximal F1 score
opt_acc = accuracy_score(identical, distances < opt_tau)

# Plot F1 score and accuracy as function of distance threshold
plt.plot(thresholds, f1_scores, label='F1 score');
plt.plot(thresholds, acc_scores, label='Accuracy');
plt.axvline(x=opt_tau, linestyle='--', lw=1, c='lightgrey', label='Threshold')
plt.title(f'Accuracy at threshold {opt_tau:.2f} = {opt_acc:.3f}');
plt.xlabel('Distance threshold')
plt.legend()
plt.show()

dist_pos = distances[identical == 1]
dist_neg = distances[identical == 0]

plt.figure(figsize=(12,4))

plt.subplot(121)
plt.hist(dist_pos)
plt.axvline(x=opt_tau, linestyle='--', lw=1, c='lightgrey', label='Threshold')
plt.title('Distances (pos. pairs)')
plt.legend();

plt.subplot(122)
plt.hist(dist_neg)
plt.axvline(x=opt_tau, linestyle='--', lw=1, c='lightgrey', label='Threshold')
plt.title('Distances (neg. pairs)')
plt.legend();
plt.show()

targets = np.array([m.name for m in metadata])

encoder = LabelEncoder()
encoder.fit(targets)

# Numerical encoding of identities
y = encoder.transform(targets)

train_idx = np.arange(metadata.shape[0]) % 2 != 0
test_idx = np.arange(metadata.shape[0]) % 2 == 0

# 50 train examples of 10 identities (5 examples each)
X_train = embedded[train_idx]
# 50 test examples of 10 identities (5 examples each)
X_test = embedded[test_idx]

y_train = y[train_idx]
y_test = y[test_idx]

knn = KNeighborsClassifier(n_neighbors=1, metric='euclidean')
svc = LinearSVC()

knn.fit(X_train, y_train)
svc.fit(X_train, y_train)

acc_knn = accuracy_score(y_test, knn.predict(X_test))
acc_svc = accuracy_score(y_test, svc.predict(X_test))

print(f'KNN accuracy = {acc_knn}, SVM accuracy = {acc_svc}')

import warnings
# Suppress LabelEncoder warning
warnings.filterwarnings('ignore')

example_idx = 100

example_image = load_image(metadata[test_idx][example_idx].image_path())
example_prediction = svc.predict([embedded[test_idx][example_idx]])
example_identity = encoder.inverse_transform(example_prediction)[0]

plt.imshow(example_image)
plt.title(f'Recognized as {example_identity}')
plt.show()


# Tran Tien Duc them vao
filename_test = '../test/BanTrung_0210.bmp'
img_test = load_image(filename_test)
img_test = align_image(img_test)
# scale RGB values to interval [0,1]
img_test = (img_test / 255.).astype(np.float32)
# obtain embedding vector for image
embedded_test = nn4_small2_pretrained.predict(np.expand_dims(img_test, axis=0))[0]

test_prediction = svc.predict([embedded_test])

mydict = ['BanChien','BanChuong','BanDoan','BanDong','BanDu','BanDucHieu','BanDuy','BanHau'
,'BanHieu','BanHoangTuan','BanHue','BanHuy','BanNa','BanNinh','BanPhat','BanPhong','BanThai','BanThang'
,'BanThanh','BanThinh','BanTinh','BanTrang','BanTrung','BanTruong','BanTu','BanTuan19','BanTuan20','BanTuyen','BanVy','ThayDuc']

result = mydict[test_prediction[0]]
print(result)

plt.imshow(img_test)
plt.title(f'Recognized as {result}')
plt.show()


X_embedded = TSNE(n_components=2).fit_transform(embedded)

for i, t in enumerate(set(targets)):
    idx = targets == t
    plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t)   

plt.legend(bbox_to_anchor=(1, 1))
plt.show()

# Kết quả sẽ tạo ra model svc.pkl
joblib.dump(svc,'svc.pkl')

B3: Tiến hành nhận dạng khuôn mặt
import sys
import tkinter
from tkinter import Frame, Tk, BOTH, Text, Menu, END
from tkinter.filedialog import Open, SaveAs

from model import create_model
import numpy as np
import os.path
import cv2
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import joblib

from align import AlignDlib
from sklearn.svm import LinearSVC

import cv2

def load_image(path):
    img = cv2.imread(path, 1)
    # OpenCV loads images with color channels
    # in BGR order. So we need to reverse them
    return img[...,::-1]

def align_image(img):
    return alignment.align(96, img, alignment.getLargestFaceBoundingBox(img), 
                           landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)

alignment = AlignDlib('models/shape_predictor_68_face_landmarks.dat')

nn4_small2_pretrained = create_model()
nn4_small2_pretrained.load_weights('weights/nn4.small2.v1.h5')

svc = joblib.load('svc.pkl')

mydict = ['BanChien','BanChuong','BanDoan','BanDong','BanDu','BanDucHieu','BanDuy','BanHau'
,'BanHieu','BanHoangTuan','BanHue','BanHuy','BanNa','BanNinh','BanPhat','BanPhong','BanThai','BanThang'
,'BanThanh','BanThinh','BanTinh','BanTrang','BanTrung','BanTruong','BanTu','BanTuan19','BanTuan20','BanTuyen','BanVy','ThayDuc']

class Main(Frame):
    
    def __init__(self, parent):
        Frame.__init__(self, parent)
        self.parent = parent
        self.initUI()
  
    def initUI(self):
        self.parent.title("Nhan Dang Khuon Mat")
        self.pack(fill=BOTH, expand=1)
  
        menubar = Menu(self.parent)
        self.parent.config(menu=menubar)
  
        fileMenu = Menu(menubar)
        fileMenu.add_command(label="Open", command=self.onOpen)
        fileMenu.add_command(label="Recognition", command=self.onRecognition)
        fileMenu.add_separator()
        fileMenu.add_command(label="Exit", command=self.quit)
        menubar.add_cascade(label="File", menu=fileMenu)
        self.txt = Text(self)
        self.txt.pack(fill=BOTH, expand=1)
  
    def onOpen(self):
        global ftypes
        ftypes = [('Images', '*.jpg *.tif *.bmp *.gif *.png')]
        dlg = Open(self, filetypes = ftypes)
        fl = dlg.show()
  
        if fl != '':
            global img
            global imgin
            imgin = cv2.imread(fl,cv2.IMREAD_COLOR)
            img = imgin[...,::-1]
            cv2.namedWindow("ImageIn", cv2.WINDOW_AUTOSIZE)
            #cv2.moveWindow("ImageIn", 200, 200)
            cv2.imshow("ImageIn", imgin)

    def onRecognition(self):
        img_test = align_image(img)
        # scale RGB values to interval [0,1]
        img_test = (img_test/255.).astype(np.float32)
        # obtain embedding vector for image
        embedded_test = nn4_small2_pretrained.predict(np.expand_dims(img_test, axis=0))[0]
        test_prediction = svc.predict([embedded_test])
        result = mydict[test_prediction[0]]
        cv2.putText(imgin,result,(5,15),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255))
        cv2.namedWindow("ImageIn", cv2.WINDOW_AUTOSIZE)
        cv2.imshow("ImageIn", imgin)

root = Tk()
Main(root)
root.geometry("480x480+100+100")
root.mainloop()



